Lab 8 Writeup
Qingxia Guo and Qingyang Wang

1/2. This week's phenomenon

This week, we decided to work on sentential negation. In Titan, sentential negation for indicative clauses is realized via
a bipartite negation strategy. There is a negation auxiliary, "ne", which comes after the mandatory number/aspect-marking
auxiliary, and before the main verb. Then, the negation word "poen" comes at the end of the clause. For example,

judgment:  g
aru ne ani kan poen
3dl neg eat food not
'They didn t eat food.'

judgment: g
ala ne alisai poen
3pl neg laugh neg.comp
'They didn't laugh'

"ne" cannot appear on its own. For example, the following are ungrammatical:

judgment: u
aru ne ani kan
3dl neg eat food
'They didn t eat food.'

judgment: u
ala ne alisai
3pl neg laugh
'They didn't laugh'

"poen" can appear on its own. However, the construction where it appears alone expresses constituent negation,
not sentential negation. which is not in the scope of the assignment for this week.

In order to represent this, we went to the customization system and added a bipartite negation strategy, where the first
part is a negative auxiliary verb, and the second part is a complement selected by said negative auxiliary verb.
This produced a new ttv.tdl file which we merged with our tdl file from last week. However, this caused some issues.

One of the main changes was the NEGATED feature, in HEAD, getting added to several phrases.
Our negative auxiliary verb "ne" was represented like this:

      aux41_neg-aux-lex := subj-raise-aux-with-pred &
        [ SYNSEM.LOCAL.CAT [ HEAD.FORM nonfinite,
                             VAL.COMPS < [ OPT -,
      				     LOCAL.CAT.HEAD.FORM nonfinite ],
      				   [ OPT -,
      				     LOCAL.CAT.HEAD.NEGATED + ] > ] ].


and the sentence-final "poen" was represented like this:

        neg-comp-lex := norm-zero-arg &
          [ SYNSEM.LOCAL [ CAT [ HEAD adv &
                                      [ NEGATED + ],
                                 VAL [ SUBJ < >,
                                       COMPS < > ] ],
                           CONT [ RELS.LIST < >,
                                  HCONS.LIST < > ] ] ].

As shown in the above snippets, "ne" takes 2 complements, one being the verb, and the other being "poen".
However, as demonstrated in demo day on Thursday, while COMPS has 2 elements, we still want ARG-ST to only have 2 elements
overall, so as not to conflict with information in matrix.tdl. Below is a snippet of subj-raise-aux-with-pred, which
aux41_neg-aux-lex inherits from. There are only 2 elements in ARG-ST.:

        subj-raise-aux-with-pred := trans-first-arg-raising-lex-item-1 & subj-raise-aux & norm-sem-lex-item &
          [ ARG-ST < [ ],
                     [ LOCAL.CONT.HOOK.LTOP #larg ] >,
            SYNSEM [ LOCAL.CONT.HCONS.LIST < qeq &
                                             [ HARG #harg,
                                               LARG #larg ] >,
                     LKEYS.KEYREL event-relation &
                                  [ ARG1 #harg ] ] ].


Finally, in subj-raise-aux, which subj-raise-aux-with-pred inherits from, COMPS is underspecified for length,
allowing aux41_neg-aux-lex to take 2 complements, but the ARG-ST is still 2 elements:

            subj-raise-aux := trans-first-arg-raising-lex-item & aux-lex &
              [ SYNSEM.LOCAL [ CAT.VAL [ SPEC < >,
                                         COMPS < #comps, ... >,
                                         SUBJ < #subj &
                                                [ LOCAL.CAT.HEAD.CASE real-case ] > ],
                               CONT.HOOK.XARG #xarg ],
                ARG-ST < #subj &
                         [ LOCAL [ CAT [ HEAD noun &
                                              [ CASE #case ],
                                         VAL [ SUBJ < >,
                                               SPR < >,
                                               SPEC < >,
                                               COMPS < > ] ],
                                   CONT.HOOK.INDEX #ind & #xarg ] ],
                         #comps &
                         [ LOCAL [ CAT [ VAL [ SUBJ < unexpressed &
                                                      [ LOCAL.CAT.HEAD.CASE #case ] >,
                                               COMPS < >,
                                               SPR < >,
                                               SPEC < > ],
                                         HEAD verb ],
                                   CONT.HOOK.XARG #ind ] ] > ].

Finally, we removed duplicate entries of "ne" and "poen" that were auto-generated by the initial choices file.
As a result of these changes, we were able to parse "aru ne ani kan poen" and "ala ne alisai poen", as expected,
each with only one parse. Additionally, as expected "ala ne alisai" failed to parse. Unexpectedly, however, "aru ne ani kan"
still parsed. We realized that this was because the sentence was analyzed as "ne" taking both "ani" and "kan" as complements.
To fix this issue, we added the feature NEGATED - to noun-lex, which all nouns inherit from. We did this because the second
complement of "ne" must be NEGATED +. By making all nouns NEGATED -, they are not allowed to be the second complement of "ne",
without attaching to a verb first. "poen" is NEGATED +, allowing it to be selected as the 2nd complement of "ne".

      noun-lex := basic-noun-lex & basic-non-wh-word-lex & non-local-none-lex-item & no-hcons-lex-item & non-mod-lex-item &
        [ ARG-ST < #spr >,
          SYNSEM.LOCAL.CAT [ VAL [ SPR < #spr &
                                         [ LOCAL.CAT.HEAD det ] >,
                                   COMPS < >,
                                   SUBJ < >,
                                   SPEC < > ],
                             HEAD [POSSESSOR nonpossessive,
      			     NEGATED -],
                             POSSESSUM nonpossessive ] ].

3. MMT cleanup

Originally, our sentence #5 was producing around 38 parses, with a bunch of semantically empty
auxiliaries in the last slot instead of poen.
Similarly to above, we added the feature NEGATED - to most of our auxiliaries:
      subj-raise-aux-no-pred := subj-raise-aux & raise-sem-lex-item &
      [ ARG-ST < [ ],
                 [OPT - ] >,
        SYNSEM.LOCAL.CAT [HEAD.NEGATED -,
              VAL.COMPS < [ ] > ]].

Afterwards, our number of parses went down to 0.

Originally, our sentence #9 produced 5 parses. Some of this ambiguity was due to the fact that Titan allows both
asyndetic and polysyndetic coordination of sentences. However, we realized that while for our asyndetic coordination
strategy, we had constrained both coordinated elements to have the same form, we had not done this for our polysyndetic
strategy. As a result, we were allowing ungrammatical generations under polysyndetic coordination. We changed our
polysyndetic strategy as follows:

      s7-top-coord-rule := basic-s-top-coord-rule & apoly-top-coord-rule &
      [ SYNSEM [LOCAL [COORD-STRAT "7",
                  CAT.HEAD.FORM #form] ],
      LCOORD-DTR.SYNSEM.LOCAL.CAT.HEAD.FORM #form,
      RCOORD-DTR.SYNSEM.LOCAL.CAT.HEAD.FORM #form ].


      s7-bottom-coord-rule := conj-first-bottom-coord-rule & s-bottom-coord-phrase &
      [ SYNSEM.LOCAL [ CAT.HEAD.FORM #form,
                   COORD-STRAT "7",
                   COORD-REL.PRED "_and_coord_rel" ],
      NONCONJ-DTR.SYNSEM.LOCAL.CAT.HEAD.FORM #form ].

After we constrained the forms of both coordinated elements, we went down to 2 parses.

Originally, for #20, we were not generating anything as our MRS did not match. For questions, we had "which_q_rel", while
the English grammar had "wh_q_rel". We changed this in our grammar's wh-pronoun-noun-lex to match:

    wh-pronoun-noun-lex := basic-wh-word-lex & norm-hook-lex-item & basic-icons-lex-item & non-mod-lex-item & zero-arg-que &
      [ SYNSEM [ LOCAL [ CAT [ HEAD noun,
                               VAL [ SPR < >,
                                     SUBJ < >,
                                     COMPS < >,
                                     SPEC < > ] ],
                         CONT [ RELS.LIST < [ LBL #larg,
                                              ARG0 #arg0 & ref-ind ],
                                            quant-relation &
                                            [ PRED "wh_q_rel",
                                              ARG0 #arg0,
                                              RSTR #harg ] >,
                                HCONS.LIST < [ HARG #harg,
                                               LARG #larg ] > ] ],
                 NON-LOCAL.QUE.LIST < #arg0 > ] ].


Originally, for #23, we were not generating anything as our MRS did not match. The important predication was that we have a
verb "va" that can mean either "say" or "think", and our predication tried to encompass all the meanings of the verb.
So, for the purposes of machine translation, we changed the predication to just "_think_v_rel":

          va_2 := verb4_trans_claus-verb-lex &
          [ STEM < "va" >,
            SYNSEM.LKEYS.KEYREL.PRED "_think_v_rel" ].

However, it seems the problem lay somewhere else as well, since we still generated 0 parses.

Originally, for #25, we generated 0 parses. We then realized that our construction for "alan", which means "because", was
not parsing at all. We then loaded an earlier grammar and realized that the rule we had removed last lab (adp-comp-head),
that we talked
about this week in class on Tuesday, actually was used for something: it was used for our "alan" construction. So, we put that
rule back in, although it would definitely reintroduce the ambiguity we were trying to get rid of last week.
This brought us up to 5 parses, but we then
realized that "alan" was sometimes taking ungrammatical (nonfinite) complements. So, we constrained "alan" to take a
finite complement:

        s-attach-clause-final-prehead-subord-lex-item := subord-with-verbal-comp-lex &
          [ SYNSEM.LOCAL.CAT [ VAL.COMPS < [ LOCAL.CAT [ HEAD.FORM finite,
        						 VAL.SUBJ < >] ] >,
                               POSTHEAD -,
                               HEAD [ MOD < [ LOCAL.CAT.VAL.SUBJ < > ] >,
                                      INIT - ] ] ].

Afterwards, we got to 2 parses.


4. Status of MMT items

#1 Dogs sleep
Works (1 output)

#2 Dogs chase cars
Works (1 output)

#3 I chase you
Works (6 parses - because "you" is underspecified for number)

#4 Dogs eat
Doesn't work - MRSes look the same, don't know what's going on

#5 The dogs dont chase cars
Doesn't work - slight difference in the MRS where the ttv MRS for our expected translation has the index of the whole
sentence identified with the arg0 of neg_rel, but the eng MRS has the index identified with the arg0 of chase_v_rel.

#6 I think that you know that dogs chase cars
Doesn't work - our grammar cannot parse multiple embedded clauses.

#7 I ask whether you know that dogs chase cars
Doesn't work - our grammar cannot parse embedded interrogative clauses.

#8 Cats and dogs chase cars
Works (5 parses - the extra parses are due to our coordination rule not constraining a value for person. As a result,
our coordinated subject can "agree" with any person pl verb number marker. We considered putting constraints on person
via the coordination rule, but weren't really sure if this would be appropriate.)

#9 Dogs chase cars and cats chase dogs
Works (2 parses - one asyndetic coordination and one polysyndeton coordination.)

#10 Cats chase dogs and sleep
Doesn't work - MRSes look the same, don't know whats going on

#11 Do cats chase dogs
Works - (1 parse, but we didn't actually expect it to parse. The generated sentence is actually the same as the
indicative statement "cats chase dogs")

#12 Hungry dogs eat
Doesn't work - our grammar doesn't handle attributive adjectives

#13 Dogs in the park eat
Doesn't work - our grammar doesn't handle attributive PPs

#14 Dogs eat in the park
Doesn't work - our grammar doesn't handle locational PPs

#15 The dogs are hungry
Doesn't work - the MRS is not the same, so we need a transfer rule

#16 The dogs are in the park
Doesn't work - the MRS is not the same, so we need a transfer rule

#17 The dogs are the cats
Works - 1 parse

#18 The dog s car sleeps
Works - 2 parses - one is the correct one, the other is wrong but we don't really know what's going on there - this
extra sentence is a result of us adding back the adp-comp-head rule (discussed above)

#19 My dogs sleep
Works - 1 parse

#20 Who sleeps
Works - 1 parse

#21 What do the dogs chase
Doesn't work - MRSes look the same, don't know whats going on

#22 What do you think the dogs chase
Doesn't work - MRSes look the same, don't know whats going on

#23 Who asked what the dogs chase
Doesn't work - our grammar doesn't handle embedded interrogative clauses.

#24 I asked what the dogs chased
Doesn't work - our grammar doesn't handle embedded interrogative clauses.

#25 The dog sleeps because the cat sleeps
Works - 2 parses - for the MRSes of the generated Titan sentences, the difference is that the arg1 and arg2 of the "because"
relation are identified with different things. Not really sure why this generated 2 sentences with opposite meanings.

#26 The dog sleeps after the cat sleeps
Doesn't work - our grammar doesn't handle this kind of construction.


5. Performance

----Intital run-------
Test corpus:
After setting a condition on sentence length, we processed 934 sentences.
number of items parsed: 177
average number of parses per parsed item: 4.9
most ambiguous item: 88
NEW sources of ambiguity: none

Testsuite:
number of items parsed: 59
average number of parses per parsed item: 9.11
most ambiguous item: 728
NEW sources of ambiguity: none


-------Final run-------
Test corpus:
After setting a condition on sentence length, we processed 934 sentences.
number of items parsed: 185
average number of parses per parsed item: 4.72
most ambiguous item: 92
NEW sources of ambiguity: Reintroduciing the adp-comp-head rule also added more ambiguity.

Testsuite:
number of items parsed: 62
average number of parses per parsed item: 2.1
most ambiguous item: 44
NEW sources of ambiguity: Our changes to the coordination rules this lab removed some ambiguity. However, the reintroduction
of the adp-comp-head rule also added some ambiguity.


6. Other changes

We fixed the noun class that contained proper names so that it was explicitly PRON -. In previous labs, this feature
not being constrained caused an issue where a proper name could go through both versions of our BARE-NP rule, both the
one for pronouns and the one for non-pronouns.

      noun68_proper_names-noun-lex := noun-lex &
        [ SYNSEM.LOCAL [ CAT.HEAD.PRON -,
                         CONT.HOOK.INDEX.PNG.PERNUM 3sg ] ].
